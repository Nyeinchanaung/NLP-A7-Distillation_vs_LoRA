# NLP-A7-Distillation_vs_LoRA
In this assignment, we will explore the comparison between Odd Layer and Even Layer Student Training Models and LoRA (Low-Rank Adaptation) on a distillation task using BERT from Huggingface.

## GitHub Link:
- https://github.com/Nyeinchanaung/NLP-A7-Distillation_vs_LoRA


## Content
- [Student Information](#student-information)
- [Files Structure](#files-structure)
- [How to run](#how-to-run)
- [Dataset](#dataset)
- [Model Training](#training)
- [Evaluation](#evaluation)
- [Web Application](#application)

## Student Information
 - Name     : Nyein Chan Aung
 - ID       : st125553
 - Program  : DSAI

## Files Structure
1) The Jupytor notebook files
- distilBERT.ipynb

2) `app` folder  
- app.py (streamlit)

 
## How to run
 - Clone the repo
 - Open the project
 - Open the `app` folder
 - `streamlit run app.py`
 - app should be up and running on `http://localhost:8501/`

## Dataset

### Source

## Training
### Hyper Parameter


### Define hyperparameter


## Evaluation

### Result


#### Key Observations:

#### Conclusion


## Application
### Application Development
### How to use web app


### Screenshots
![Webapp1](ss1.png)
![Webapp1](ss2.png)
![Webapp1](ss3.png)
![Webapp2](ss4.gif)